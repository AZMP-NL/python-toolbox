'''
To generate AZMP score cards for bottom temperature

Uses pickled object generated by azmp_bottom_stats.py

Check /home/cyrf0006/AZMP/SAR_files

Files generated:
BT_2J_fall.dat
BT_3K_fall.dat
BT_3LNO_fall.dat
BT_3LNO_spring.dat
BT_3Ps_spring.dat
CIL_Bonavista_0C_Area.dat
CIL_FlemishCap_0C_Area.dat
CIL_SealIsland_0C_Area.dat
CIL_WhiteBay_0C_Area.dat
S27_Integrated.dat
'''

import numpy as  np
import matplotlib.pyplot as plt
import pandas as pd
import os
#import unicodedata

# Parameters 
path = '/home/cyrf0006/AZMP/state_reports/bottomT/'
clim_year = [1991, 2020]
year_min = 1948
stn27_months = [5, 11]

#### -------------1. bottom temperature ---------------- ####
## 2J fall
infile = path + 'stats_2J_fall.pkl'
df = pd.read_pickle(infile)
df.index = pd.to_datetime(df.index) # update index to datetime
# Flag bad years (no or weak sampling):
bad_years = np.array([1995])
for i in bad_years:
    df[df.index.year==i]=np.nan
# compute std anom
df_clim = df[(df.index.year>=clim_year[0]) & (df.index.year<=clim_year[1])]
std_anom = (df-df_clim.mean(axis=0))/df_clim.std(axis=0)
# std anom for temperature
df['std_anom'] = std_anom['Tmean']
# keep only 2 columns
df = df[['Tmean', 'std_anom']]
df.index = df.index.year
df.to_csv('BT_2J_fall.dat', header=False, sep = ' ', float_format='%.2f')

## 3K fall
infile = path + 'stats_3K_fall.pkl'
df = pd.read_pickle(infile)
df.index = pd.to_datetime(df.index) # update index to datetime
# Flag bad years (no or weak sampling):
bad_years = np.array([])
for i in bad_years:
    df[df.index.year==i]=np.nan
# compute std anom
df_clim = df[(df.index.year>=clim_year[0]) & (df.index.year<=clim_year[1])]
std_anom = (df-df_clim.mean(axis=0))/df_clim.std(axis=0)
# std anom for temperature
df['std_anom'] = std_anom['Tmean']
# keep only 2 columns
df = df[['Tmean', 'std_anom']]
df.index = df.index.year
df.to_csv('BT_3K_fall.dat', header=False, sep = ' ', float_format='%.2f')

## 3LNO fall
infile = path + 'stats_3LNO_fall.pkl'
df = pd.read_pickle(infile)
df.index = pd.to_datetime(df.index) # update index to datetime
# Flag bad years (no or weak sampling):
bad_years = np.array([2021])
for i in bad_years:
    df[df.index.year==i]=np.nan
# compute std anom
df_clim = df[(df.index.year>=clim_year[0]) & (df.index.year<=clim_year[1])]
std_anom = (df-df_clim.mean(axis=0))/df_clim.std(axis=0)
# std anom for temperature
df['std_anom'] = std_anom['Tmean']
# keep only 2 columns
df = df[['Tmean', 'std_anom']]
df.index = df.index.year
df.to_csv('BT_3LNO_fall.dat', header=False, sep = ' ', float_format='%.2f')

## 3LNO spring
infile = path + 'stats_3LNO_spring.pkl'
df = pd.read_pickle(infile)
df.index = pd.to_datetime(df.index) # update index to datetime
# Flag bad years (no or weak sampling):
bad_years = np.array([2020, 2021])
for i in bad_years:
    df[df.index.year==i]=np.nan
# compute std anom
df_clim = df[(df.index.year>=clim_year[0]) & (df.index.year<=clim_year[1])]
std_anom = (df-df_clim.mean(axis=0))/df_clim.std(axis=0)
# std anom for temperature
df['std_anom'] = std_anom['Tmean']
# keep only 2 columns
df = df[['Tmean', 'std_anom']]
df.index = df.index.year
df.to_csv('BT_3LNO_spring.dat', header=False, sep = ' ', float_format='%.2f')

## 3Ps spring 
infile = path + 'stats_3Ps_spring.pkl'
df = pd.read_pickle(infile)
df.index = pd.to_datetime(df.index) # update index to datetime
# Flag bad years (no or weak sampling):
bad_years = np.array([1980, 1981, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 2006, 2020])
for i in bad_years:
    df[df.index.year==i]=np.nan
# compute std anom
df_clim = df[(df.index.year>=clim_year[0]) & (df.index.year<=clim_year[1])]
std_anom = (df-df_clim.mean(axis=0))/df_clim.std(axis=0)
# std anom for temperature
df['std_anom'] = std_anom['Tmean']
# keep only 2 columns
df = df[['Tmean', 'std_anom']]
df.index = df.index.year
df.to_csv('BT_3Ps_spring.dat', header=False, sep = ' ', float_format='%.2f')

#### ------------- 2. winter NAO ---------------- ####
## nao_file = '/home/cyrf0006/data/AZMP/indices/data.csv'
## df = pd.read_csv(nao_file, header=1)
## # Set index
## df = df.set_index('Date')
## df.index = pd.to_datetime(df.index, format='%Y%m')
## # Select only DJF
## df_winter = df[(df.index.month==12) | (df.index.month==1) | (df.index.month==2) |  (df.index.month==3)]
## # Start Dec-1950
## df_winter = df_winter[df_winter.index>pd.to_datetime('1950-10-01')]
## # Average 3 consecutive values (DJF average); We loose index.
## df_winter = df_winter.groupby(np.arange(len(df_winter))//4).mean()
## # Reset index using years only
## year_unique = pd.unique(df.index.year)[1:,]
## df_winter = df_winter.iloc[np.arange(0, year_unique.size)] # reduce if last month is december (belongs to following year)
## df_winter.index = year_unique
## df_winter.to_csv('NAO_DJFM.dat', header=False, sep = ' ', float_format='%.2f')

df_winter = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/airTemp/NAO_winter.pkl')


#### ------------- 3. CIL ---------------- ####
# see /home/cyrf0006/AZMP/state_reports/ColbourneStuff/CIL_AZMP_SPRING_SUMMER_FALL.xlsx
# These timeseries are already calculated for the IROC (except for WB). 
# Check iroc_CIL_area.py and files in /home/cyrf0006/research/WGOH/IROC <-- Should be removed

# UPDATE 2022 check: 
# azmp_CIL_stats.py
# azmp_CIL_stats_update.pu

df_SI = pd.read_csv('/home/cyrf0006/AZMP/state_reports/sections_plots/CIL/CIL_area_SI.csv')
df_WB = pd.read_csv('/home/cyrf0006/AZMP/state_reports/sections_plots/CIL/CIL_area_WB.csv')
df_BB = pd.read_csv('/home/cyrf0006/AZMP/state_reports/sections_plots/CIL/CIL_area_BB.csv')
df_FC = pd.read_csv('/home/cyrf0006/AZMP/state_reports/sections_plots/CIL/CIL_area_FC.csv')
df_SI.rename(columns={df_SI.columns[0]:'year'}, inplace=True)
df_WB.rename(columns={df_WB.columns[0]:'year'}, inplace=True)
df_BB.rename(columns={df_BB.columns[0]:'year'}, inplace=True)
df_FC.rename(columns={df_FC.columns[0]:'year'}, inplace=True)
df_SI.set_index('year', inplace=True)
df_WB.set_index('year', inplace=True)
df_BB.set_index('year', inplace=True)
df_FC.set_index('year', inplace=True) 
# cut timeseries
df_SI = df_SI[df_SI.index>=year_min]
df_WB = df_WB[df_WB.index>=year_min]
df_BB = df_BB[df_BB.index>=year_min]
df_FC = df_FC[df_FC.index>=year_min]
# Save timeseries
df_SI['interp_field'].to_csv('CIL_SealIsland_0C_Area.dat', header=False, sep = ' ', float_format='%.2f')
df_WB['interp_field'].to_csv('CIL_WhiteBay_0C_Area.dat', header=False, sep = ' ', float_format='%.2f')
df_BB['interp_field'].to_csv('CIL_Bonavista_0C_Area.dat', header=False, sep = ' ', float_format='%.2f')
df_FC['interp_field'].to_csv('CIL_FlemishCap_0C_Area.dat', header=False, sep = ' ', float_format='%.2f')

df_SI['station-ID'].to_csv('CIL_SealIsland_0C_Area_stationBased.dat', header=False, sep = ' ', float_format='%.2f')
df_WB['station-ID'].to_csv('CIL_WhiteBay_0C_Area_stationBased.dat', header=False, sep = ' ', float_format='%.2f')
df_BB['station-ID'].to_csv('CIL_Bonavista_0C_Area_stationBased.dat', header=False, sep = ' ', float_format='%.2f')
df_FC['station-ID'].to_csv('CIL_FlemishCap_0C_Area_stationBased.dat', header=False, sep = ' ', float_format='%.2f')



#### ------------- 4. Stn 27 ---------------- ####
# see /home/cyrf0006/AZMP/S27/station_27_stratification.xlsx
# Load pickled data
df_temp = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_temperature_monthly.pkl')
df_sal = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_salinity_monthly.pkl')
df_strat = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_stratif_monthly.pkl')


# Reduce to summer months and annual mean
df_temp = df_temp[(df_temp.index.month>=stn27_months[0]) & (df_temp.index.month<=stn27_months[1])]
df_sal = df_sal[(df_sal.index.month>=stn27_months[0]) & (df_sal.index.month<=stn27_months[1])]
df_strat = df_strat[(df_strat.index.month>=stn27_months[0]) & (df_strat.index.month<=stn27_months[1])]


# Choose depth range:
# Temperature
T_0_btm = df_temp.mean(axis=1)
T_0_50 = df_temp[df_temp.columns[(df_temp.columns<=50)]].mean(axis=1)
T_170_btm = df_temp[df_temp.columns[(df_temp.columns>=170)]].mean(axis=1)
# Salinity
S_0_btm = df_sal.mean(axis=1)
S_0_50 = df_sal[df_sal.columns[(df_sal.columns<=50)]].mean(axis=1)
S_170_btm = df_sal[df_sal.columns[(df_sal.columns>=170)]].mean(axis=1)
# Statification
strat_5_50 = df_strat*42 # the SAR presents density difference, not stratification


# Recreate annual cycle based on climatology and anomaly
series_name_in = ['T_0_btm', 'T_0_50', 'T_170_btm', 'S_0_btm', 'S_0_50', 'S_170_btm', 'strat_5_50']
series_name_out = ['out_T_0_btm', 'out_T_0_50', 'out_T_170_btm', 'out_S_0_btm', 'out_S_0_50', 'out_S_170_btm', 'out_strat_5_50']
for idx, i in enumerate(series_name_in):
    my_ts = eval(i)
    ts_stack = my_ts.groupby([(my_ts.index.year),(my_ts.index.month)]).mean()
    ts_unstack = ts_stack.unstack()
    # Monthly clim (ts_monthly_clim)
    ts_clim_period = my_ts[(my_ts.index.year>=clim_year[0]) & (my_ts.index.year<=clim_year[1])]
    ts_monthly_stack = ts_clim_period.groupby([(ts_clim_period.index.year),(ts_clim_period.index.month)]).mean()
    ts_monthly_clim = ts_monthly_stack.groupby(level=1).mean()
    ts_monthly_std = ts_monthly_stack.groupby(level=1).std()
    # monthly anom and normalized anom
    monthly_anom = ts_unstack - ts_monthly_clim 
    monthly_stdanom = (ts_unstack - ts_monthly_clim) /  ts_monthly_std
    # annual normalized anomaly
    anom_std = monthly_stdanom.mean(axis=1)
    anom_std.index = pd.to_datetime(anom_std.index, format='%Y')
    # annual anomaly
    anom = monthly_anom.mean(axis=1) 
    anom.index = pd.to_datetime(anom.index, format='%Y')
    # Re-create annual mean by adding annual anomaly to monthly clim
    annual_mean = anom + ts_monthly_clim.mean()
    exec(series_name_out[idx] + ' = annual_mean')


# Merge 
df_stn27 = pd.concat([out_T_0_50, out_T_170_btm, out_S_0_50, out_strat_5_50], axis=1, keys=['Temp 0-50m', 'Temp 170-176m', 'Sal 0-50m', 'Strat 5-50m'])
df_stn27.index = df_stn27.index.year
df_stn27 = df_stn27[df_stn27.index>=year_min]
df_stn27.to_csv('S27_Integrated.dat', header=True, sep = ' ', na_rep='-99', float_format='%.3f')



# in Linux:  zip SAR_azmp-nl_2019.zip *.dat

